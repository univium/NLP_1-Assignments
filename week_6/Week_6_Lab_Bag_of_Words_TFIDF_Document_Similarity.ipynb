{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0rz0poNLMX8S"
   },
   "source": [
    "# Lab - Bag of Words, TFIDF, Document Similarity\n",
    "\n",
    "## Lab Summary:\n",
    "\n",
    "In this lab we will be learning about document similarity and word embeddings.\n",
    "## Lab Goal:\n",
    "Upon completion of this lab, the student should be able to:\n",
    "<ul>\n",
    "    <li> understand cosine, jaccard, and Euclidean similarity </li>\n",
    "    <li> test if 2 documents are similar or not </li>\n",
    "</ul>\n",
    "\n",
    "\n",
    "## Time Requried:\n",
    "This lab should take 4 hours to complete for those unfamiliar with the tools.\n",
    "Hint: The lab flow is already prepared for you, just read through the links and try implementing them. Don't try to have it all figured out before starting.\n",
    "\n",
    "## Lab Preparation:\n",
    "<ul>\n",
    "    <li> Review Lecture for Week 5 and 6 </li>\n",
    "</ul>\n",
    "\n",
    "## Hardware Needed:\n",
    "Any computer with access to the internet and web browser\n",
    "\n",
    "\n",
    "\n",
    "## Import Packages and Classes (Initial)\n",
    "\n",
    "In this lab we will be using the following libraries:\n",
    "<ol>\n",
    "    <li> NLTK </li>\n",
    "    <li> Pandas </li>\n",
    "    <li> Matplotlib </li>\n",
    "    <li> Gensim </li>\n",
    "</ol>\n",
    "\n",
    "\n",
    "<b>Matplotlib</b> Matplotlib is a comprehensive library for creating static, animated, and interactive visualizations in Python. It is completely free and open source.\n",
    "\n",
    "![Image](https://miro.medium.com/max/1050/1*EsqDYFK-IzGEAm4FyZP0wQ.jpeg)\n",
    "\n",
    "More here: https://matplotlib.org/stable/index.\n",
    "\n",
    "<b>Pandas</b> is a fast, powerful, flexible and easy to use open source data analysis and manipulation tool, built on top of the Python programming language.\n",
    "\n",
    "![Image](https://miro.medium.com/max/819/1*Dss7A8Z-M4x8LD9ccgw7pQ.png)\n",
    "\n",
    "More here: https://pandas.pydata.org/\n",
    "\n",
    "<b>Gensim</b> supports a variety of other NLP tasks such as: \n",
    "- converting words to vectors (word2vec), \n",
    "- document to vectors (doc2vec), \n",
    "- finding text similarity, and text summarization\n",
    "\n",
    "![Gensim](https://repository-images.githubusercontent.com/1349775/202c4680-8f7c-11e9-91c6-745fdcbeffe8)\n",
    "\n",
    "More about Gensim here:\n",
    "https://pypi.org/project/gensim/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tiPqiK6lXHno"
   },
   "source": [
    "<b>Bag of Words</b> is a Natural Language Processing technique of text modelling. A bag of words is a representation of text that describes the occurrence of words within a document. \n",
    "\n",
    "At first, we would see how to find the bag of words representation without using any library. Then, we would use sklearn library to achieve the same thing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "ybu5sUvX8-Ei"
   },
   "outputs": [],
   "source": [
    "def vectorize(tokens):\n",
    "    ''' This function returns the bag of words representation. You need to use this function ahead'''\n",
    "    vector=[]\n",
    "    for w in filtered_vocab:\n",
    "        vector.append(tokens.count(w))\n",
    "        #tokens.count(w) tells us the count of each word in the filtered_vocab in tokens\n",
    "        #filtered_vocab contains list of unique words after filtering stopwords and punctuation\n",
    "    return vector\n",
    "\n",
    "    \n",
    "def unique(sequence):\n",
    "    '''this function returns the vocabulary of words'''\n",
    "    seen = set()\n",
    "    return set(sequence) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "QkKto_t39f3C",
    "outputId": "e6563680-008d-4180-80bd-9048fc98cf53"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['paris', 'is', 'the', 'capital', 'of', 'france']\n",
      "['milan', 'is', 'the', 'fashion', 'capital', 'of', 'the', 'world']\n"
     ]
    }
   ],
   "source": [
    "stopwords=[\"to\",\"is\",\"a\",\"the\",\"of\"]\n",
    "special_char=[\",\",\":\",\" \",\";\",\".\",\"?\"]\n",
    "\n",
    "\n",
    "#your list of sentences\n",
    "string1=\"Paris is the capital of France\"\n",
    "string2=\"Milan is the fashion capital of the world\"\n",
    "\n",
    "#lowercasing is one of the most important steps in text preprocessing. A particular word whether in lower or upper\n",
    "#case means the same thing\n",
    "#convert them to lower case\n",
    "#your code here\n",
    "string1=string1.lower()\n",
    "string2=string2.lower()\n",
    "\n",
    "#your code here\n",
    "#split the sentences into tokens\n",
    "tokens1=string1.split()\n",
    "tokens2=string2.split()\n",
    "\n",
    "\n",
    "print(tokens1)\n",
    "print(tokens2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dq6g_Y1zr_A8"
   },
   "source": [
    "Now, we will find the vocabulary which is the set of unique words in the corpus. We also need to filter it since we do not want stopwords or special characters to influence our vocabulary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "mSVJfDPE9f6_",
    "outputId": "1a1c7996-a47a-42aa-d140-ad93151ce335"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'capital', 'fashion', 'france', 'is', 'milan', 'of', 'paris', 'the', 'world'}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab=unique(tokens1+tokens2)\n",
    "vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ZDYRbZb59f9i",
    "outputId": "c4b83f9b-1950-4889-ff6e-c762b8875e2e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['milan', 'fashion', 'capital', 'france', 'paris', 'world']\n"
     ]
    }
   ],
   "source": [
    "filtered_vocab=[]\n",
    "#filtered_vocab should contain the words from vocab that are not stopwords or special_char\n",
    "#your code here\n",
    "for w in vocab: \n",
    "    if w not in stopwords and w not in special_char: \n",
    "        filtered_vocab.append(w)\n",
    "print(filtered_vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "hkhVniRK9gAy",
    "outputId": "fba85ed5-1d36-4270-85d1-8771df98e65a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['milan', 'fashion', 'capital', 'france', 'paris', 'world']\n",
      "[0, 0, 1, 1, 1, 0]\n",
      "[1, 1, 1, 0, 0, 1]\n"
     ]
    }
   ],
   "source": [
    "print(filtered_vocab)\n",
    "#convert sentences into vectords\n",
    "vector1=vectorize(tokens1)\n",
    "print(vector1)\n",
    "vector2=vectorize(tokens2)\n",
    "print(vector2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "itYF6KLmEhi5"
   },
   "source": [
    "Well, that was the bag of words, the hard way! Now, we would do the same using the sklearn library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "UHdZ5TdTW_FO",
    "outputId": "aa72aaa1-8ff0-432c-87c7-d85cbe380b8a"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Sweden is part of the geographical area of Fennoscandia',\n",
       " ' The climate is in general mild for its northerly latitude due to significant maritime influence',\n",
       " ' In spite of the high latitude, Sweden often has warm continental summers, being located in between the North Atlantic, the Baltic Sea, and vast Russia',\n",
       " ' The general climate and environment vary significantly from the south and north due to the vast latitudal difference, and much of Sweden has reliably cold and snowy winters',\n",
       " ' Southern Sweden is predominantly agricultural, while the north is heavily forested and includes a portion of the Scandinavian Mountains',\n",
       " '']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text= \"Sweden is part of the geographical area of Fennoscandia. The climate is in general mild for its northerly latitude due to significant maritime influence. In spite of the high latitude, Sweden often has warm continental summers, being located in between the North Atlantic, the Baltic Sea, and vast Russia. The general climate and environment vary significantly from the south and north due to the vast latitudal difference, and much of Sweden has reliably cold and snowy winters. Southern Sweden is predominantly agricultural, while the north is heavily forested and includes a portion of the Scandinavian Mountains.\"\n",
    "#Split text into sentences. Store the sentences into a list\n",
    "#Your code here:\n",
    "text = text.split('.')\n",
    "text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hhjO6YfGMX8f"
   },
   "source": [
    "We would use the sklearn library to find bag of word representation automatically.\n",
    "\n",
    "The CountVectorizer provides a simple way to both tokenize a collection of text documents and build a vocabulary of known words, but also to encode new documents using that vocabulary. \n",
    "\n",
    "\n",
    "More about this here: https://machinelearningmastery.com/prepare-text-data-machine-learning-scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "6xQbIfM4MX8f",
    "outputId": "5ace57d5-e2c2-471e-cb72-e3ae2a0be462"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   agricultural  area  atlantic  baltic  climate  cold  continental  \\\n",
      "0             0     1         0       0        0     0            0   \n",
      "1             0     0         0       0        1     0            0   \n",
      "2             0     0         1       1        0     0            1   \n",
      "3             0     0         0       0        1     1            0   \n",
      "4             1     0         0       0        0     0            0   \n",
      "5             0     0         0       0        0     0            0   \n",
      "\n",
      "   difference  environment  fennoscandia  ...  snowy  south  southern  spite  \\\n",
      "0           0            0             1  ...      0      0         0      0   \n",
      "1           0            0             0  ...      0      0         0      0   \n",
      "2           0            0             0  ...      0      0         0      1   \n",
      "3           1            1             0  ...      1      1         0      0   \n",
      "4           0            0             0  ...      0      0         1      0   \n",
      "5           0            0             0  ...      0      0         0      0   \n",
      "\n",
      "   summers  sweden  vary  vast  warm  winters  \n",
      "0        0       1     0     0     0        0  \n",
      "1        0       0     0     0     0        0  \n",
      "2        1       1     0     1     1        0  \n",
      "3        0       1     1     1     0        1  \n",
      "4        0       1     0     0     0        0  \n",
      "5        0       0     0     0     0        0  \n",
      "\n",
      "[6 rows x 43 columns]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "CountVec = CountVectorizer(\n",
    "    ngram_range=(1,1),  # use (2,2) for bigrams\n",
    "    stop_words='english'\n",
    ")\n",
    "\n",
    "Count_data = CountVec.fit_transform(sentence for sentence in text)\n",
    "\n",
    "cv_dataframe = pd.DataFrame(\n",
    "    Count_data.toarray(),\n",
    "    columns=CountVec.get_feature_names_out()\n",
    ")\n",
    "\n",
    "print(cv_dataframe)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "b4VCKtdl8NcN"
   },
   "source": [
    "The next task is to find the bag of words representation again but this time, do it for <b>bigrams</b>!\n",
    "\n",
    "A bigram or digram is a sequence of two adjacent elements from a string of tokens, which are typically letters, syllables, or words. A bigram is an n-gram for n=2. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "TT3kTBUtoVP2",
    "outputId": "2f2eb157-6061-4d5a-d9d9-98a6b7db6b69"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   agricultural north  area fennoscandia  atlantic baltic  baltic sea  \\\n",
      "0                   0                  1                0           0   \n",
      "1                   0                  0                0           0   \n",
      "2                   0                  0                1           1   \n",
      "3                   0                  0                0           0   \n",
      "4                   1                  0                0           0   \n",
      "5                   0                  0                0           0   \n",
      "\n",
      "   climate environment  climate general  cold snowy  continental summers  \\\n",
      "0                    0                0           0                    0   \n",
      "1                    0                1           0                    0   \n",
      "2                    0                0           0                    1   \n",
      "3                    1                0           1                    0   \n",
      "4                    0                0           0                    0   \n",
      "5                    0                0           0                    0   \n",
      "\n",
      "   difference sweden  environment vary  ...  spite high  summers located  \\\n",
      "0                  0                 0  ...           0                0   \n",
      "1                  0                 0  ...           0                0   \n",
      "2                  0                 0  ...           1                1   \n",
      "3                  1                 1  ...           0                0   \n",
      "4                  0                 0  ...           0                0   \n",
      "5                  0                 0  ...           0                0   \n",
      "\n",
      "   sweden geographical  sweden predominantly  sweden reliably  sweden warm  \\\n",
      "0                    1                     0                0            0   \n",
      "1                    0                     0                0            0   \n",
      "2                    0                     0                0            1   \n",
      "3                    0                     0                1            0   \n",
      "4                    0                     1                0            0   \n",
      "5                    0                     0                0            0   \n",
      "\n",
      "   vary significantly  vast latitudal  vast russia  warm continental  \n",
      "0                   0               0            0                 0  \n",
      "1                   0               0            0                 0  \n",
      "2                   0               0            1                 1  \n",
      "3                   1               1            0                 0  \n",
      "4                   0               0            0                 0  \n",
      "5                   0               0            0                 0  \n",
      "\n",
      "[6 rows x 47 columns]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "CountVec = CountVectorizer(\n",
    "    ngram_range=(2,2),   # bigrams\n",
    "    stop_words='english'\n",
    ")\n",
    "\n",
    "Count_data = CountVec.fit_transform(sentence for sentence in text)\n",
    "\n",
    "cv_dataframe = pd.DataFrame(\n",
    "    Count_data.toarray(),\n",
    "    columns=CountVec.get_feature_names_out()\n",
    ")\n",
    "\n",
    "print(cv_dataframe)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QnC-bALoMX8g"
   },
   "source": [
    "<b>TFIDF</b>\n",
    "\n",
    "Term frequency–inverse document frequency, is a numerical statistic that is intended to reflect how important a word is to a document in a collection or corpus\n",
    "\n",
    "![Image](https://miro.medium.com/max/3604/1*qQgnyPLDIkUmeZKN2_ZWbQ.png)\n",
    "\n",
    "You can read more about TFIDF  here: http://www.tfidf.com/, https://programminghistorian.org/en/lessons/analyzing-documents-with-tfidf\n",
    "\n",
    "Now, let's implement tfidf first without libraries and then with them!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "FMQPM75_MX8h"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3jRr5PHeMX8i"
   },
   "source": [
    "For this section we would be working with three documents:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "FUkPHjSTMX8j"
   },
   "outputs": [],
   "source": [
    "doc1 = 'When does summer begin , When'\n",
    "doc2 = 'The rain soothes my soul'\n",
    "doc3 = 'The winter is lovely but long'\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Jxrj1VSSMX8l"
   },
   "source": [
    "Our corpus here is three documents: doc1, doc2, and doc3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "NHPXmTLXp9GB",
    "outputId": "9cd233cf-e68c-4118-9d01-08ea78dc3eb5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['When', 'does', 'summer', 'begin', ',', 'When']\n",
      "['The', 'rain', 'soothes', 'my', 'soul']\n",
      "['The', 'winter', 'is', 'lovely', 'but', 'long']\n"
     ]
    }
   ],
   "source": [
    "#split each of the three documents into tokens. Store the tokens from doc1 in a variable bowDOC1\n",
    "#Store the tokens from doc2 in a variable bowDOC2. Store the tokens from doc3 in a variable bowDOC3\n",
    "#your code here\n",
    "bowDOC1 = doc1.split(' ')\n",
    "bowDOC2 = doc2.split(' ')\n",
    "bowDOC3 = doc3.split(' ')\n",
    "print(bowDOC1)\n",
    "print(bowDOC2)\n",
    "print(bowDOC3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "lx4id3KUkOZG",
    "outputId": "d5b8a51e-fa36-4c39-b4d4-3c2c39894ec2"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{',',\n",
       " 'The',\n",
       " 'When',\n",
       " 'begin',\n",
       " 'but',\n",
       " 'does',\n",
       " 'is',\n",
       " 'long',\n",
       " 'lovely',\n",
       " 'my',\n",
       " 'rain',\n",
       " 'soothes',\n",
       " 'soul',\n",
       " 'summer',\n",
       " 'winter'}"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Remember the vocabulary is the set of unique words from all the documents\n",
    "#find the vocabulary by finding the unique words of bowDOC1, bowDOC2, and bowDOC3\n",
    "#your code here\n",
    "vocabulary = set(bowDOC1).union(set(bowDOC2)).union(set(bowDOC3))\n",
    "vocabulary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sJKemUT4uJ_w"
   },
   "source": [
    "Now, we have the vocabulary- the set of unique words. We need to convert our documents into vectors so that we can pass these vectors on to our machines for any sort of computation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "kOL_YsXuJ3T5",
    "outputId": "150a8f3c-268d-4bfe-f125-4514a3bdf0ae"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'soothes': 0, ',': 0, 'long': 0, 'The': 0, 'soul': 0, 'rain': 0, 'winter': 0, 'begin': 0, 'my': 0, 'When': 0, 'is': 0, 'lovely': 0, 'but': 0, 'does': 0, 'summer': 0}\n",
      "{'soothes': 0, ',': 1, 'long': 0, 'The': 0, 'soul': 0, 'rain': 0, 'winter': 0, 'begin': 1, 'my': 0, 'When': 2, 'is': 0, 'lovely': 0, 'but': 0, 'does': 1, 'summer': 1}\n"
     ]
    }
   ],
   "source": [
    "#finding the bow vector for doc1\n",
    "vectorA = dict.fromkeys(vocabulary, 0)\n",
    "print(vectorA)\n",
    "for word in bowDOC1:\n",
    "    vectorA[word] += 1\n",
    "print(vectorA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "k-5RbgQxJ3Xj",
    "outputId": "9ca3ef6f-6c00-42c3-e75d-360d083cd564"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'soothes': 0, ',': 0, 'long': 0, 'The': 0, 'soul': 0, 'rain': 0, 'winter': 0, 'begin': 0, 'my': 0, 'When': 0, 'is': 0, 'lovely': 0, 'but': 0, 'does': 0, 'summer': 0}\n",
      "{'soothes': 1, ',': 0, 'long': 0, 'The': 1, 'soul': 1, 'rain': 1, 'winter': 0, 'begin': 0, 'my': 1, 'When': 0, 'is': 0, 'lovely': 0, 'but': 0, 'does': 0, 'summer': 0}\n"
     ]
    }
   ],
   "source": [
    "#find the bow vector for doc2\n",
    "#your code here\n",
    "vectorB = dict.fromkeys(vocabulary, 0)\n",
    "print(vectorB)\n",
    "for word in bowDOC2:\n",
    "    vectorB[word] += 1\n",
    "print(vectorB)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "qBFvbuutJ3aN",
    "outputId": "49e5c284-5ba8-43e1-ca50-d33201e17bcd"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'soothes': 0, ',': 0, 'long': 0, 'The': 0, 'soul': 0, 'rain': 0, 'winter': 0, 'begin': 0, 'my': 0, 'When': 0, 'is': 0, 'lovely': 0, 'but': 0, 'does': 0, 'summer': 0}\n",
      "{'soothes': 0, ',': 0, 'long': 1, 'The': 1, 'soul': 0, 'rain': 0, 'winter': 1, 'begin': 0, 'my': 0, 'When': 0, 'is': 1, 'lovely': 1, 'but': 1, 'does': 0, 'summer': 0}\n"
     ]
    }
   ],
   "source": [
    "#find the bow vector for doc3\n",
    "#your code here\n",
    "vectorC = dict.fromkeys(vocabulary, 0)\n",
    "print(vectorC)\n",
    "for word in bowDOC3:\n",
    "    vectorC[word] += 1\n",
    "print(vectorC)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Xvih1cWIurIL"
   },
   "source": [
    "TFIDF consists of 2 steps: finding the TF and the IDF. The final result is just the product of TF and IDF. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "id": "bACWMbCJJ3cU"
   },
   "outputs": [],
   "source": [
    "#this is a function for computing term frequency\n",
    "def computeTF(wordDict, bagOfWords):\n",
    "    tfDict = {}\n",
    "    bagOfWordsCount = len(bagOfWords)#finds the length of list bagOfWords\n",
    "    for word, count in wordDict.items():\n",
    "        tfDict[word] = count / float(bagOfWordsCount) #finding term frequency\n",
    "    return tfDict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "id": "PbLptcH0J3e8"
   },
   "outputs": [],
   "source": [
    "#term frquency for doc1\n",
    "tfA = computeTF(vectorA, bowDOC1)\n",
    "#find the term frequencies for doc2 and doc3, store them in variables tfB and tfC\n",
    "tfB = computeTF(vectorB, bowDOC2)\n",
    "tfC = computeTF(vectorC, bowDOC3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "zWVrZ5c-J3ha",
    "outputId": "c1eb27e5-b5e2-488a-f819-e889cd335f02"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'soothes': 0.2,\n",
       " ',': 0.0,\n",
       " 'long': 0.0,\n",
       " 'The': 0.2,\n",
       " 'soul': 0.2,\n",
       " 'rain': 0.2,\n",
       " 'winter': 0.0,\n",
       " 'begin': 0.0,\n",
       " 'my': 0.2,\n",
       " 'When': 0.0,\n",
       " 'is': 0.0,\n",
       " 'lovely': 0.0,\n",
       " 'but': 0.0,\n",
       " 'does': 0.0,\n",
       " 'summer': 0.0}"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Just seeing how the tf for document B looks like\n",
    "tfB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "id": "S86vGfDTLa3A"
   },
   "outputs": [],
   "source": [
    "#function for computing inverse document frequency\n",
    "def computeIDF(documents):\n",
    "    import math\n",
    "    N = len(documents)\n",
    "    \n",
    "    idfDict = dict.fromkeys(documents[0].keys(), 0)\n",
    "    for document in documents:\n",
    "        for word, val in document.items():\n",
    "            if val > 0:\n",
    "                idfDict[word] += 1\n",
    "    \n",
    "    for word, val in idfDict.items():\n",
    "        idfDict[word] = math.log(N / float(val))\n",
    "    return idfDict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "IH4ZKdIeLa9I",
    "outputId": "496947a3-3344-4b64-af27-94995a2b793f"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'soothes': 1.0986122886681098,\n",
       " ',': 1.0986122886681098,\n",
       " 'long': 1.0986122886681098,\n",
       " 'The': 0.4054651081081644,\n",
       " 'soul': 1.0986122886681098,\n",
       " 'rain': 1.0986122886681098,\n",
       " 'winter': 1.0986122886681098,\n",
       " 'begin': 1.0986122886681098,\n",
       " 'my': 1.0986122886681098,\n",
       " 'When': 1.0986122886681098,\n",
       " 'is': 1.0986122886681098,\n",
       " 'lovely': 1.0986122886681098,\n",
       " 'but': 1.0986122886681098,\n",
       " 'does': 1.0986122886681098,\n",
       " 'summer': 1.0986122886681098}"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "idfs = computeIDF([vectorA, vectorB, vectorC])\n",
    "idfs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "id": "tUj_kJg2La-_"
   },
   "outputs": [],
   "source": [
    "#tfidf is just tf*idf\n",
    "def computeTFIDF(tfBagOfWords, idfs):\n",
    "    tfidf = {}\n",
    "    for word, val in tfBagOfWords.items():\n",
    "        tfidf[word] = val * idfs[word]\n",
    "    return tfidf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "id": "CxOfwqfMLbBV"
   },
   "outputs": [],
   "source": [
    "#find the tfidf for 3 documents and represent them in a data frame\n",
    "#your code here\n",
    "tfidfA = computeTFIDF(tfA, idfs)\n",
    "tfidfB = computeTFIDF(tfB, idfs)\n",
    "tfidfC = computeTFIDF(tfC, idfs)\n",
    "\n",
    "df = pd.DataFrame([tfidfA, tfidfB, tfidfC])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 161
    },
    "id": "gT09wWKVLbEA",
    "outputId": "b35611ce-bf44-4d19-866a-b6a75af9f7ac"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>soothes</th>\n",
       "      <th>,</th>\n",
       "      <th>long</th>\n",
       "      <th>The</th>\n",
       "      <th>soul</th>\n",
       "      <th>rain</th>\n",
       "      <th>winter</th>\n",
       "      <th>begin</th>\n",
       "      <th>my</th>\n",
       "      <th>When</th>\n",
       "      <th>is</th>\n",
       "      <th>lovely</th>\n",
       "      <th>but</th>\n",
       "      <th>does</th>\n",
       "      <th>summer</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.183102</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.183102</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.366204</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.183102</td>\n",
       "      <td>0.183102</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.219722</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.081093</td>\n",
       "      <td>0.219722</td>\n",
       "      <td>0.219722</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.219722</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.183102</td>\n",
       "      <td>0.067578</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.183102</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.183102</td>\n",
       "      <td>0.183102</td>\n",
       "      <td>0.183102</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    soothes         ,      long       The      soul      rain    winter  \\\n",
       "0  0.000000  0.183102  0.000000  0.000000  0.000000  0.000000  0.000000   \n",
       "1  0.219722  0.000000  0.000000  0.081093  0.219722  0.219722  0.000000   \n",
       "2  0.000000  0.000000  0.183102  0.067578  0.000000  0.000000  0.183102   \n",
       "\n",
       "      begin        my      When        is    lovely       but      does  \\\n",
       "0  0.183102  0.000000  0.366204  0.000000  0.000000  0.000000  0.183102   \n",
       "1  0.000000  0.219722  0.000000  0.000000  0.000000  0.000000  0.000000   \n",
       "2  0.000000  0.000000  0.000000  0.183102  0.183102  0.183102  0.000000   \n",
       "\n",
       "     summer  \n",
       "0  0.183102  \n",
       "1  0.000000  \n",
       "2  0.000000  "
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KW9I9Q6-rsS2"
   },
   "source": [
    "Now, let's find tfidf using scikit-learn."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "M_OlOIVtNCU3",
    "outputId": "041af1dd-367b-4f98-a321-74125ca5385a"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>begin</th>\n",
       "      <th>but</th>\n",
       "      <th>does</th>\n",
       "      <th>is</th>\n",
       "      <th>long</th>\n",
       "      <th>lovely</th>\n",
       "      <th>my</th>\n",
       "      <th>rain</th>\n",
       "      <th>soothes</th>\n",
       "      <th>soul</th>\n",
       "      <th>summer</th>\n",
       "      <th>the</th>\n",
       "      <th>when</th>\n",
       "      <th>winter</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.377964</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.377964</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.377964</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.755929</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.467351</td>\n",
       "      <td>0.467351</td>\n",
       "      <td>0.467351</td>\n",
       "      <td>0.467351</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.355432</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.423394</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.423394</td>\n",
       "      <td>0.423394</td>\n",
       "      <td>0.423394</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.322002</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.423394</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      begin       but      does        is      long    lovely        my  \\\n",
       "0  0.377964  0.000000  0.377964  0.000000  0.000000  0.000000  0.000000   \n",
       "1  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  0.467351   \n",
       "2  0.000000  0.423394  0.000000  0.423394  0.423394  0.423394  0.000000   \n",
       "\n",
       "       rain   soothes      soul    summer       the      when    winter  \n",
       "0  0.000000  0.000000  0.000000  0.377964  0.000000  0.755929  0.000000  \n",
       "1  0.467351  0.467351  0.467351  0.000000  0.355432  0.000000  0.000000  \n",
       "2  0.000000  0.000000  0.000000  0.000000  0.322002  0.000000  0.423394  "
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# define tf-idf vectorizer\n",
    "vectorizer = TfidfVectorizer()\n",
    "\n",
    "# fit documents into vectorizer\n",
    "vectors = vectorizer.fit_transform([doc1, doc2, doc3])\n",
    "\n",
    "# get vocabulary\n",
    "feature_names = vectorizer.get_feature_names_out()\n",
    "\n",
    "\n",
    "# convert to readable format\n",
    "dense = vectors.todense()\n",
    "denselist = dense.tolist()\n",
    "\n",
    "# print the tf-idf vectors\n",
    "df = pd.DataFrame(denselist, columns=feature_names)\n",
    "df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ik4A14RiNcg_"
   },
   "source": [
    "The values slightly differ because sklearn uses a slightly different implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 191
    },
    "id": "X3MpAUIuNbfq",
    "outputId": "33249b4c-b7a0-45dd-ef74-1ec713dc029f"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>20</th>\n",
       "      <th>and</th>\n",
       "      <th>at</th>\n",
       "      <th>bails</th>\n",
       "      <th>ball</th>\n",
       "      <th>bat</th>\n",
       "      <th>batting</th>\n",
       "      <th>being</th>\n",
       "      <th>between</th>\n",
       "      <th>bowled</th>\n",
       "      <th>...</th>\n",
       "      <th>team</th>\n",
       "      <th>teams</th>\n",
       "      <th>test</th>\n",
       "      <th>the</th>\n",
       "      <th>to</th>\n",
       "      <th>twenty20</th>\n",
       "      <th>two</th>\n",
       "      <th>when</th>\n",
       "      <th>wicket</th>\n",
       "      <th>with</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.245646</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.198871</td>\n",
       "      <td>0.245646</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.31157</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.31157</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.31157</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.224511</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.143302</td>\n",
       "      <td>0.177007</td>\n",
       "      <td>0.177007</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.177007</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.708028</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.224511</td>\n",
       "      <td>0.177007</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.193204</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.245054</td>\n",
       "      <td>0.156415</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.245054</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.193204</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.579611</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.245054</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.217618</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.171572</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.217618</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.217618</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.217618</td>\n",
       "      <td>0.217618</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.171572</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>4 rows × 50 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         20       and        at     bails      ball       bat   batting  \\\n",
       "0  0.000000  0.245646  0.000000  0.000000  0.198871  0.245646  0.000000   \n",
       "1  0.000000  0.000000  0.224511  0.000000  0.143302  0.177007  0.177007   \n",
       "2  0.000000  0.193204  0.000000  0.245054  0.156415  0.000000  0.000000   \n",
       "3  0.217618  0.000000  0.000000  0.000000  0.000000  0.000000  0.171572   \n",
       "\n",
       "      being  between    bowled  ...      team    teams      test       the  \\\n",
       "0  0.000000  0.31157  0.000000  ...  0.000000  0.31157  0.000000  0.000000   \n",
       "1  0.000000  0.00000  0.177007  ...  0.000000  0.00000  0.000000  0.708028   \n",
       "2  0.245054  0.00000  0.193204  ...  0.000000  0.00000  0.000000  0.579611   \n",
       "3  0.000000  0.00000  0.000000  ...  0.217618  0.00000  0.217618  0.000000   \n",
       "\n",
       "         to  twenty20      two      when    wicket      with  \n",
       "0  0.000000  0.000000  0.31157  0.000000  0.000000  0.000000  \n",
       "1  0.000000  0.000000  0.00000  0.000000  0.224511  0.177007  \n",
       "2  0.000000  0.000000  0.00000  0.245054  0.000000  0.000000  \n",
       "3  0.217618  0.217618  0.00000  0.000000  0.000000  0.171572  \n",
       "\n",
       "[4 rows x 50 columns]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc1 = 'Cricket is a bat-and-ball game played between two teams of eleven players'\n",
    "doc2 = 'The batting side scores runs by striking the ball bowled at the wicket with the bat '\n",
    "doc3 = 'Means of dismissal include being bowled, when the ball hits the stumps and dislodges the bails'\n",
    "doc4 = 'Forms of cricket range from Twenty20, with each team batting for a single innings of 20 overs, to Test matches played over five days'\n",
    "#find the tfidf representation for each of these documents\n",
    "#your code here\n",
    "vectorizer = TfidfVectorizer()\n",
    "vectors = vectorizer.fit_transform([doc1, doc2, doc3, doc4])\n",
    "feature_names = vectorizer.get_feature_names_out()\n",
    "dense = vectors.todense()\n",
    "denselist = dense.tolist()\n",
    "df = pd.DataFrame(denselist, columns=feature_names)\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_aVOL_8hTdjs"
   },
   "source": [
    "# Document Similarity\n",
    "\n",
    "- Text Similarity has to determine how the two text documents close to each other\n",
    "\n",
    "- The similarity can be in terms of both context and meaning\n",
    "\n",
    "- For finding similarity, text needs to be converted into vectors\n",
    "\n",
    "- There are various text similarity metric exist such as Cosine similarity, Jaccard Similarity.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rFGUEGSW-GKe"
   },
   "source": [
    "#Cosine Similarity\n",
    "\n",
    "- Cosine similarity is a metric used to measure how similar 2 documents are \n",
    "- Mathematically, it measures the cosine of the angle between two vectors projected in a multi-dimensional space. \n",
    "- The smaller the angle, higher the cosine similarity.\n",
    "\n",
    "![Image](https://datascience-enthusiast.com/figures/cosine_sim.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "id": "__NqSdhTTc0Q"
   },
   "outputs": [],
   "source": [
    "doc_1 = \"Brazil won the Football world cup five times\" \n",
    "doc_2 = \"Italy comes after Brazil in that regard\" \n",
    "\n",
    "#find the bag of word vector representation\n",
    "CountVec = CountVectorizer(ngram_range=(1,1))\n",
    "Count_data = CountVec.fit_transform(sentence for sentence in [doc_1, doc_2])\n",
    "vectorA = Count_data.toarray()[0]\n",
    "vectorB = Count_data.toarray()[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "L6X3PLlBBPOW"
   },
   "source": [
    "We would now calculate the cosine similarity between documents using Sklearn."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 110
    },
    "id": "SkZOW5YmTVes",
    "outputId": "ebbeb2b7-ff7f-46f2-9eb9-774aa5587150"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>doc_1</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.133631</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>doc_2</th>\n",
       "      <td>0.133631</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              0         1\n",
       "doc_1  1.000000  0.133631\n",
       "doc_2  0.133631  1.000000"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "#use the cosine_similarity function to calculate cosine distance like this\n",
    "cosine_similarity_matrix = cosine_similarity(Count_data)\n",
    "pd.DataFrame(cosine_similarity_matrix,['doc_1','doc_2'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "id": "GHXHDpbvVyxE"
   },
   "outputs": [],
   "source": [
    "doc_1 = \"Sweden is in Scandanavia\" \n",
    "doc_2 = \"Denmark is a neighbor of Sweden\" \n",
    "doc_3 = \"Norway and Denmark are close by\"\n",
    "#find the pairwise cosine similarity between the documents\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 110
    },
    "id": "xADVY4iu9MJP",
    "outputId": "3a79b1df-c47c-473a-e233-8efab6082a48"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>doc_1</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.447214</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>doc_2</th>\n",
       "      <td>0.447214</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              0         1\n",
       "doc_1  1.000000  0.447214\n",
       "doc_2  0.447214  1.000000"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#your code here\n",
    "CountVec = CountVectorizer(ngram_range=(1,1))\n",
    "Count_data = CountVec.fit_transform(sentence for sentence in [doc_1, doc_2])\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "cosine_similarity_matrix = cosine_similarity(Count_data)\n",
    "pd.DataFrame(cosine_similarity_matrix,['doc_1','doc_2'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 110
    },
    "id": "GRlWqIZY9p9T",
    "outputId": "b5409884-49c5-492b-87f7-91b8b2015263"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>doc_2</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.182574</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>doc_3</th>\n",
       "      <td>0.182574</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              0         1\n",
       "doc_2  1.000000  0.182574\n",
       "doc_3  0.182574  1.000000"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#your code here\n",
    "CountVec = CountVectorizer(ngram_range=(1,1))\n",
    "Count_data = CountVec.fit_transform(sentence for sentence in [doc_2, doc_3])\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "cosine_similarity_matrix = cosine_similarity(Count_data)\n",
    "pd.DataFrame(cosine_similarity_matrix,['doc_2','doc_3'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 110
    },
    "id": "jaWsQF6Y9wDD",
    "outputId": "3100c596-d2da-4409-81a2-d7660989f398"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>doc_1</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>doc_3</th>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         0    1\n",
       "doc_1  1.0  0.0\n",
       "doc_3  0.0  1.0"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#your code here\n",
    "CountVec = CountVectorizer(ngram_range=(1,1))\n",
    "Count_data = CountVec.fit_transform(sentence for sentence in [doc_1, doc_3])\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "cosine_similarity_matrix = cosine_similarity(Count_data)\n",
    "pd.DataFrame(cosine_similarity_matrix,['doc_1','doc_3'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "andfzSnG-Ey8"
   },
   "source": [
    "#Jaccard Similarity\n",
    "\n",
    "- Jaccard Similarity is also known as the Jaccard index and Intersection over Union \n",
    "- used to determine the similarity between two text document in terms of their context \n",
    "- similarity is in terms of how many common words are exist over total words\n",
    "\n",
    "![Image](https://miro.medium.com/max/744/1*XiLRKr_Bo-VdgqVI-SvSQg.png)\n",
    "\n",
    "More here: https://en.wikipedia.org/wiki/Jaccard_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "id": "9HxZXmfs91Ug"
   },
   "outputs": [],
   "source": [
    "# Let's have 2 documents\n",
    "\n",
    "doc1 = 'A is the brother of B'\n",
    "doc2 = 'B is the friend of C who is not a brother of A'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "id": "GkzcIHzpCLGN"
   },
   "outputs": [],
   "source": [
    "#convert documents to lower case - basic preprocessing!\n",
    "doc1 = doc1.lower()\n",
    "doc2 = doc2.lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "id": "WDGGxg98Doc3"
   },
   "outputs": [],
   "source": [
    "#split both the documents into tokens. Make sure that you have no duplicates. You might want to use sets for this purpose!?\n",
    "#your code here\n",
    "doc1 = set(doc1.split())\n",
    "doc2 = set(doc2.split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "id": "TC9UGMnGC59r"
   },
   "outputs": [],
   "source": [
    "#find common words from the 2 documents\n",
    "#your code here\n",
    "intersection = doc1.intersection(doc2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "id": "PbEB770eDzZJ"
   },
   "outputs": [],
   "source": [
    "#find the vocabulary, total unique words in both documents\n",
    "#your code here\n",
    "union = doc1.union(doc2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ZcfjZWD5EEjw",
    "outputId": "c86da5f6-1610-451b-81f9-141a1fc058b5"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#find jaccard similarity\n",
    "#your code here\n",
    "float(len(intersection)) / len(union)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_EzpJKERRg_C"
   },
   "source": [
    "\n",
    "# Submission Instructions\n",
    "    1. Run all code cells in the notebook\n",
    "\n",
    "    2. Answer all questions in markdown cells beneath each section\n",
    "\n",
    "    3. Export the notebook as HTML:\n",
    "\n",
    "        File → Download as → HTML (.html)\n",
    "\n",
    "    4. Submit the HTML file in Canvas by the posted deadline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sWnncIrtEV9m"
   },
   "source": [
    "## Setup\n",
    "### Run the following cell before starting the assignment:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Libraries loaded successfully!\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from gensim.models import KeyedVectors\n",
    "import gensim.downloader as api\n",
    "\n",
    "# For better plot display in HTML export\n",
    "import plotly.express as px\n",
    "import plotly.io as pio\n",
    "pio.renderers.default = \"svg\"\n",
    "\n",
    "print(\"Libraries loaded successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "documents = [\n",
    "    \"Hindi cinema, often known as Bollywood, is the Indian Hindi-language film industry based in Mumbai.\",\n",
    "    \"Films adapted from comic books have had plenty of success, whether about superheroes like Batman and Superman or geared toward kids.\",\n",
    "    \"Every now and then a movie comes along from a suspect studio that becomes a critical darling despite low expectations.\",\n",
    "    \"The Year 2000 problem, also known as Y2K, refers to computer bugs related to date formatting in the new millennium.\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Doc 1</th>\n",
       "      <th>Doc 2</th>\n",
       "      <th>Doc 3</th>\n",
       "      <th>Doc 4</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Doc 1</th>\n",
       "      <td>1.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.166</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Doc 2</th>\n",
       "      <td>0.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>0.069</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Doc 3</th>\n",
       "      <td>0.000</td>\n",
       "      <td>0.069</td>\n",
       "      <td>1.000</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Doc 4</th>\n",
       "      <td>0.166</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>1.000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       Doc 1  Doc 2  Doc 3  Doc 4\n",
       "Doc 1  1.000  0.000  0.000  0.166\n",
       "Doc 2  0.000  1.000  0.069  0.000\n",
       "Doc 3  0.000  0.069  1.000  0.000\n",
       "Doc 4  0.166  0.000  0.000  1.000"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectorizer = TfidfVectorizer(lowercase=True)#, stop_words='english')\n",
    "tfidf_matrix = vectorizer.fit_transform(documents)\n",
    "\n",
    "similarity_matrix = cosine_similarity(tfidf_matrix)\n",
    "\n",
    "df_sim = pd.DataFrame(\n",
    "    similarity_matrix.round(3),\n",
    "    index=[f\"Doc {i+1}\" for i in range(4)],\n",
    "    columns=[f\"Doc {i+1}\" for i in range(4)]\n",
    ")\n",
    "\n",
    "df_sim"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Answer the following in markdown cells:\n",
    "    1. Which pair of documents has the highest similarity score? Does this result make intuitive sense? Explain why or why not.\n",
    "    \n",
    "    2. Why is the similarity between Document 1 and Document 4 close to zero?\n",
    "    \n",
    "    3. Re-run the analysis without removing stop words (stop_words=None). How does the similarity matrix change, and why?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. \n",
    "Looking at the table produced, it seems that the documents of 1 & 4 have some degree of similarity; the rest do not have any besides their digonal accross. On face value this doens't make much intutive sense due to the subject matter being competely different. To understand why this is the case, the matter of Cosine Similiarity comes into play. Such an algorithm operates on the stem of words rather then that of their meaning (symantic operations); this can mean that documents which have common stems belonging to words will have a greater degree of similarity evne if the meaning doens't appear the same.\n",
    "\n",
    "Here whaat was passed wasn't the pure vectorization of the corpus but rather of the output of the tfidfvectorizer; this means such a matrix had it's vectors tuned to have common words be within ideal differeanting directions. This would then make the use of cosine similiarty here so as to find stem-similarity between rare words within the corpus.\n",
    "\n",
    "#### 2.\n",
    "Though being the only two which are similar, their values are close to zero. This would be here to mean the rare words within the corpus have only a small amount of stem-similarity. \n",
    "\n",
    "#### 3. \n",
    "Doing this the cosine similarity scores go up for documents 1 & 4 while also now getting a score for documents 3 & 2. This would mean there would be a rare amount of stop words witin the corpus which have the same stem throughout as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Download a smaller, fast model (first run may take 1–2 minutes)\n",
    "# Options include:\n",
    "# 'glove-wiki-gigaword-50'\n",
    "# 'word2vec-google-news-300'\n",
    "# 'fasttext-wiki-news-subwords-300'\n",
    "\n",
    "model = api.load(\"glove-wiki-gigaword-100\")\n",
    "print(\"Model loaded! Vocabulary size:\", len(model))\n",
    "\n",
    "words = ['paris', 'italy', 'man', 'woman', 'car', 'bike', 'apple', 'banana']\n",
    "\n",
    "for word in words:\n",
    "    if word in model:\n",
    "        similar = model.most_similar(word, topn=3)\n",
    "        print(f\"\\n{word.upper()}:\")\n",
    "        for w, score in similar:\n",
    "            print(f\"  {w} ({score:.3f})\")\n",
    "\n",
    "def document_vector(doc):\n",
    "    words = [w.lower() for w in doc.split() if w.lower() in model]\n",
    "    if len(words) == 0:\n",
    "        return np.zeros(model.vector_size)\n",
    "    return np.mean(model[words], axis=0)\n",
    "\n",
    "doc_vectors = [document_vector(doc) for doc in documents]\n",
    "embedding_similarity = cosine_similarity(doc_vectors)\n",
    "\n",
    "df_emb_sim = pd.DataFrame(\n",
    "    embedding_similarity.round(3),\n",
    "    index=[f\"Doc {i+1}\" for i in range(4)],\n",
    "    columns=[f\"Doc {i+1}\" for i in range(4)]\n",
    ")\n",
    "\n",
    "df_emb_sim"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2\n",
    "### Load a Pre-Trained Embedding Model\n",
    "\n",
    "### Word-Level Similarity\n",
    "\n",
    "### Document-Level Similarity Using Embeddings\n",
    "\n",
    "### Answer the following:\n",
    "        1. What pattern do you notice between king → queen and man → woman?\n",
    "\n",
    "        2. Try your own analogy (for example: \"paris\" - \"france\" + \"italy\"). Does the result return something close to \"rome\"?\n",
    "\n",
    "        3. Compare this similarity matrix with the TF-IDF matrix from Part 1. Which method better captures semantic similarity versus keyword overlap?\n",
    "\n",
    "        4. Why might Document 2 and Document 3 show higher similarity with embeddings than with TF-IDF?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.\n",
    "For both the simliarity analysis of Queen & King there is a second order to what are commonly understood linearities to their significant other; instead, the first simliarites to these terms are rather what they are before in their status being prince & princess. When looking at the simliarity analysis of Man there can be seen a differnce from the above terms: instead of to a term to denote what a man once was it's instead to the term Women.\n",
    "\n",
    "### 2. \n",
    "Chainging the King & Queen terms to the terms Pairs & Italy doens't result in a similarity between neither of them for the term Rome; rather, I get other terms like that of Frace for Paris and Spain for Italy.\n",
    "\n",
    "\n",
    "### 3.\n",
    "Focusing on the difference between the use of TF-IDF matrix and the model used here it would show that the model used here has a matrix output which is more human readable where it presents the most simliary terms to the user. In part 1 rather it requires a manual inspection for what terms are more similar to each other. In view of keyword overlap, these sorts of models provide a greater degree of possible simliarity between terms rather then that of a binary simliarity.\n",
    "\n",
    "\n",
    "### 4.\n",
    "I'm not sure what document of 2 & 3 the question is refering to, but this likely can be a general question to ask about why embedding might discover higher simlarites then TF-IDF.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 3: Reflection and Real-World Application (30 points)\n",
    "\n",
    "Recommended reading (optional but helpful):\n",
    "\n",
    "https://intellica-ai.medium.com/comparison-of-different-word-embeddings-on-text-similarity-a-use-case-in-nlp-e83e08469c1cLinks to an external site.\n",
    "\n",
    "### Answer each question in 3–5 sentences:\n",
    "\n",
    "    1. Provide two practical business examples where accurate text similarity improves workforce productivity (for example: customer support, legal review, content recommendation).\n",
    "\n",
    "    2. Why do word embeddings generally outperform TF-IDF for semantic tasks, while TF-IDF may still be preferred for speed or interpretability?\n",
    "\n",
    "    3. If you were building a plagiarism detection system for student essays, which approach would you favor and why?\n",
    "\n",
    "    4. Based on this lab, which embedding model would you try next (Word2Vec, FastText, or BERT), and for what reason?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.\n",
    "Going with the example given for legal review, I think it's common to think of a sort of replacement for a legal review using such methods of NLP, but putting these tools in the hands of such a reviewer can rather differentiate them via their perticuler ablity. One might have a trend within their set of documents they can hone into by possibly using a basic suite which can perform TF-IDF, thereby finding the use of important terms.\n",
    "\n",
    "## 2.\n",
    "The reasoning for TF-IDF being sightly less preformative then Word Embedding models comes about from at least two things. The first is that TF-IDF is built from the foundation of similarity based on semantics rather then lexical, which means terms that share similair form are what's related within TF-IDF and that terms which had shared simliar meaning within sentences are what's related when using Embedding models. The second is that word embedding models map within lower dimensions whihch means there can be denser vectors of simliarites for the model to learn from; inverse TF-IDF maps within very high dimensions which can cause greater distnace between vectors. \n",
    "\n",
    "\n",
    "## 3.\n",
    "The idea of plagiarism brings to me that idea of someone using simliar ideas in a similar way; the only reason I think this could be is due to the effort it takes to re-write something in another way resulting in just learning the material anyways. This means I would use a word embedding model for finding the lexical similairty of sentences to paragraphs, perhaps looking into the herustics of hyperparameter tuning.\n",
    "\n",
    "\n",
    "## 4.\n",
    "Trying out FastText would be interesting due to it's approach of representing terms as bags of character of n-grams rather then that of atmoic units belonging to the term. This itself may show that it's a herustic of doing such a pre-processing/embedding style for similiarity matching.\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "Lab_Bag_of_Words,_TFIDF,_Document_Similarity.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
